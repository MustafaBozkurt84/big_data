{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[2]\") \\\n",
    ".appName(\"Caching and Persistence of Data\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Caching and Persistence of Data</h2>\n",
    "\n",
    "<p>What is the difference between caching and persistence? In Spark <strong>they are synonymous</strong>.\n",
    "Two API calls, cache() and persist(), offer these capabilities. The latter provides\n",
    "more control over how and where your data is storedâ€”in memory and on disk,\n",
    "serialized and unserialized. Both contribute to better performance for frequently\n",
    "accessed DataFrames or tables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", F.col(\"id\") * F.col(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.161103\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df.cache()\n",
    "df.count() # Materialize the cache\n",
    "stop = dt.datetime.now()\n",
    "print( (stop - start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.558911\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df.count()\n",
    "stop = dt.datetime.now()\n",
    "print( (stop - start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_ui_storage_cached_data.png\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chache() works with the first action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.range(1 * 20000000).toDF(\"id\").withColumn(\"square\", F.col(\"id\") * F.col(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, square: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.persist(StorageLevel.DISK_ONLY) # Serialize the data and cache it on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.079018\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df2.count() \n",
    "stop = dt.datetime.now()\n",
    "print( (stop - start).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648386\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df2.count() # Now get it from the cache\n",
    "stop = dt.datetime.now()\n",
    "print( (stop - start).total_seconds())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_persist_ui_size_on_disk.png\" />\n",
    "\n",
    "<p>Pay attention that df2 is cached on disk while df is on memory</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To unpersist your cached data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, square: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.274264\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df2.count() # Now get it from the cache\n",
    "stop = dt.datetime.now()\n",
    "print( (stop - start).total_seconds() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>persist(StorageLevel.LEVEL) is nuanced, providing control over how your data is \n",
    "cached via StorageLevel</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/dataframe_persist_storage_level.png\" />\n",
    "\n",
    "Source: Learning Spark, O'Reilly, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
