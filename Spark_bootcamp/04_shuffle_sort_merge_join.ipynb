{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Similar to relational databases, the Spark DataFrame and Dataset APIs\n",
    "and Spark SQL offer a series of join transformations: inner joins, outer joins, left\n",
    "joins, right joins, etc. All of these operations trigger a large amount of data movement\n",
    "across Spark executors.</p>\n",
    "\n",
    "<h3>Five join strategiy</h3>\n",
    "<ol>\n",
    "    <li>The broadcast hash join (BHJ)</li>\n",
    "    <li>Shuffle hahs join (SHJ)</li>\n",
    "    <li>Shuffle sort merge join (SMJ)</li>\n",
    "    <li>Broadcast nested loop join (BNLJ)</li>\n",
    "    <li>Shuffle and replicated nested loop join (Castesian product)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> By default Spark will use a <strong>broadcast join</strong> if the smaller data set is less than 10 MB. <br>\n",
    "This configuration is set in spark.sql.autoBroadcastJoinThreshold; <br>\n",
    "# By setting this value to -1 broadcasting can be disabled. <br>\n",
    "    We use <strong>Sort Merge Joins if we are joining two big tables.</strong></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 473232\r\n",
      "-rw-rw-r--. 1 train train  42658497 Nov  3 22:07 201508_trip_data.csv\r\n",
      "-rw-rw-r--. 1 train train      4556 Jul 21 18:58 Advertising.csv\r\n",
      "drwxrwxr-x. 2 train train      4096 Nov 10 22:02 cat_images\r\n",
      "-rw-rw-r--. 1 train train    674857 Sep 17 22:49 Churn_Modelling.csv\r\n",
      "drwxrwxr-x. 3 train train        96 Oct  6 12:18 churn-telecom\r\n",
      "-rw-rw-r--. 1 train train  41002480 Oct  6 12:18 Fire_Incidents.csv.gz\r\n",
      "drwxrwxr-x. 7 train train        67 Oct  6 12:18 flight-data\r\n",
      "-rw-rw-r--. 1 train train  46401315 Oct  6 12:18 Hotel_Reviews.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Sep 17 20:53 hotel_reviews_parquet\r\n",
      "-rw-rw-r--. 1 train train       180 Sep  2 11:40 insanlar.csv\r\n",
      "-rw-rw-r--. 1 train train      4611 Sep  1 16:13 iris.csv\r\n",
      "-rw-rw-r--. 1 train train     15802 Sep  1 16:13 iris.json\r\n",
      "-rw-rw-r--. 1 train train    325145 Sep  1 16:14 kuruyemisler.txt\r\n",
      "-rw-rw-r--. 1 train train  44525776 Oct  6 12:18 market1mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Nov 10 22:17 market1mil_snappyparquet\r\n",
      "-rw-rw-r--. 1 train train 269015852 Oct  6 12:18 market5mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Nov 10 22:16 market5mil_snappyparquet\r\n",
      "-rw-rw-r--. 1 train train      7149 Sep  4 11:15 mixed_tc_kimlik_names_numbers.txt\r\n",
      "-rw-r--r--. 1 train train       844 Oct 25 10:25 movies.json\r\n",
      "-rw-rw-r--. 1 train train   1072063 Sep 29 14:58 online_shoppers_intention.csv\r\n",
      "-rw-rw-r--. 1 train train  38382007 Oct  6 12:18 pump_sensor_data.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       133 Sep  2 12:13 retail_db\r\n",
      "-rw-rw-r--. 1 train train      7908 Sep  4 10:04 sample_tc_kimlik_names_numbers.txt\r\n",
      "drwxrwxr-x. 2 train train       112 Sep 29 18:35 sf-airbnb-clean.parquet\r\n",
      "-rw-rw-r--. 1 train train       223 Sep  2 13:03 simple_text.txt\r\n",
      "drwxrwxr-x. 2 train train        49 Oct  6 12:18 weblogs\r\n",
      "-rw-rw-r--. 1 train train    449173 Sep  3 22:32 words.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ~/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "SparkSession.builder.appName(\"Joins\").master(\"local[2]\")\n",
    "    .config(\"spark.executor.memory\",\"3g\")\n",
    "    .config(\"spark.driver.memory\",\"512m\")\n",
    "    .config(\"spark.memory.fraction\",\"0.1\")\n",
    "    .config(\"spark.memory.storageFraction\",\"0.0\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't anything against it the small df lt 10 MB will is broadcasted\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Sort Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevented_bcast_join_df = categories.join(departments, \n",
    "                                categories.categoryDepartmentId == departments.departmentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [categoryDepartmentId#17], [departmentId#54], Inner\n",
      ":- *(2) Sort [categoryDepartmentId#17 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(categoryDepartmentId#17, 200), true, [id=#166]\n",
      ":     +- *(1) Project [categoryId#16, categoryDepartmentId#17, categoryName#18]\n",
      ":        +- *(1) Filter isnotnull(categoryDepartmentId#17)\n",
      ":           +- FileScan csv [categoryId#16,categoryDepartmentId#17,categoryName#18] Batched: false, DataFilters: [isnotnull(categoryDepartmentId#17)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/categories.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(categoryDepartmentId)], ReadSchema: struct<categoryId:int,categoryDepartmentId:int,categoryName:string>\n",
      "+- *(4) Sort [departmentId#54 ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(departmentId#54, 200), true, [id=#175]\n",
      "      +- *(3) Project [departmentId#54, departmentName#55]\n",
      "         +- *(3) Filter isnotnull(departmentId#54)\n",
      "            +- FileScan csv [departmentId#54,departmentName#55] Batched: false, DataFilters: [isnotnull(departmentId#54)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/departments.csv.gz], PartitionFilters: [], PushedFilters: [IsNotNull(departmentId)], ReadSchema: struct<departmentId:int,departmentName:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prevented_bcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
