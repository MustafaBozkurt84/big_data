{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      spark-submit \\\n",
    "      --class <main-class> \\\n",
    "      --master <master-url> \\\n",
    "      --deploy-mode <deploy-mode> \\\n",
    "      --conf <key>=<value> \\\n",
    "      ... # other options\n",
    "      <application-jar> or <apllication.py> \\\n",
    "      [application-arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's submit a simple Spark application that read data and print 20 rows to screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put /home/train/datasets/market5mil_snappyparquet/ /user/train/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (venvspark) [train@localhost 05_submit_spark_applications]$ spark-submit \\\n",
    "    --master yarn \\\n",
    "    --deploy-mode client \\\n",
    "    simple_submit.py \\\n",
    "    -i file:///user/train/datasets/market5mil_snappyparquet \\\n",
    "    -f parquet \\\n",
    "    -c snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12\r\n",
      "-rw-rw-r--. 1 train train 7043 Nov 12 21:18 01_spark-submit.ipynb\r\n",
      "-rw-rw-r--. 1 train train 1818 Nov  5 19:31 simple_submit.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit with configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (venvspark) [train@localhost 05_submit_spark_applications]$ spark-submit \\\n",
    "        --master yarn \\\n",
    "        --deploy-mode client \\\n",
    "        --num-executors 2 \\\n",
    "        --conf spark.sql.autoBroadcastJoinThreshold=-1 \\\n",
    "        simple_submit.py \\\n",
    "        -i file:///home/train/datasets/market5mil_snappyparquet \\\n",
    "        -f parquet \\\n",
    "        -c snappy \\\n",
    "        --appName SparkApplicationSubmit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit with extra packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (venvspark) [train@localhost 05_submit_spark_applications]$ spark-submit \\\n",
    "        --master yarn \\\n",
    "        --deploy-mode client \\\n",
    "        --num-executors 2 \\\n",
    "        --packages \"org.apache.spark:spark-avro_2.12:3.0.0\" \\\n",
    "        --conf spark.sql.autoBroadcastJoinThreshold=-1 \\\n",
    "        most_selling_products_to_postgresql.py \\\n",
    "        -i hdfs://localhost:9000/user/train/datasets/market5mil_snappyparquet \\\n",
    "        -f parquet \\\n",
    "        -c snappy \\\n",
    "        --appName SparkApplicationSubmit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After submit you see the following logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    :: resolving dependencies :: org.apache.spark#spark-submit-parent-630b8af2-4eaf-4f64-a2de-15b84345333e;1.0\n",
    "            confs: [default]\n",
    "            found org.apache.spark#spark-avro_2.12;3.0.0 in central\n",
    "            found org.spark-project.spark#unused;1.0.0 in central\n",
    "    :: resolution report :: resolve 950ms :: artifacts dl 22ms\n",
    "            :: modules in use:\n",
    "            org.apache.spark#spark-avro_2.12;3.0.0 from central in [default]\n",
    "            org.spark-project.spark#unused;1.0.0 from central in [default]\n",
    "            ---------------------------------------------------------------------\n",
    "            |                  |            modules            ||   artifacts   |\n",
    "            |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
    "            ---------------------------------------------------------------------\n",
    "            |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
    "            ---------------------------------------------------------------------\n",
    "    :: retrieving :: org.apache.spark#spark-submit-parent-630b8af2-4eaf-4f64-a2de-15b84345333e\n",
    "            confs: [default]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit with extra jars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (venvspark) [train@localhost 05_submit_spark_applications]$ spark-submit \\\n",
    "            --master yarn \\\n",
    "            --deploy-mode client \\\n",
    "            --num-executors 2 \\\n",
    "            --jars \"hdfs://localhost:9000/tmp/extra-jars/org.apache.spark_spark-avro_2.12-3.0.0.jar\" \\\n",
    "            --conf spark.sql.autoBroadcastJoinThreshold=-1 \\\n",
    "            most_selling_products_to_postgresql.py \\\n",
    "            -i hdfs://localhost:9000/user/train/datasets/market5mil_snappyparquet \\\n",
    "            -f parquet \\\n",
    "            -c snappy \\\n",
    "            --appName SparkApplicationSubmit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--jars.\n",
    "\n",
    "Spark uses the following URL scheme to allow different strategies for disseminating jars:\n",
    "\n",
    "<strong>file:</strong> - Absolute paths and file:/ URIs are served by the driverâ€™s HTTP file server, and every executor pulls the file from the driver HTTP server.<br>\n",
    "<strong>hdfs:</strong>, http:, https:, ftp: - these pull down files and JARs from the URI as expected <br>\n",
    "<strong>local:</strong> - a URI starting with local:/ is expected to exist as a local file on each worker node. This means that no network IO will be incurred, and works well for large files/JARs that are pushed to each worker, or shared via NFS, GlusterFS, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
