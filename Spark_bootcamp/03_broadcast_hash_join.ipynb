{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Similar to relational databases, the Spark DataFrame and Dataset APIs\n",
    "and Spark SQL offer a series of join transformations: inner joins, outer joins, left\n",
    "joins, right joins, etc. All of these operations trigger a large amount of data movement\n",
    "across Spark executors.</p>\n",
    "\n",
    "<h3>Five join strategiy</h3>\n",
    "<ol>\n",
    "    <li>The broadcast hash join (BHJ)</li>\n",
    "    <li>Shuffle hahs join (SHJ)</li>\n",
    "    <li>Shuffle sort merge join (SMJ)</li>\n",
    "    <li>Broadcast nested loop join (BNLJ)</li>\n",
    "    <li>Shuffle and replicated nested loop join (Castesian product)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default Spark will use a broadcast join if the smaller data set is less than 10 MB. \n",
    "# This configuration is set in spark.sql.autoBroadcastJoinThreshold;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 391556\r\n",
      "drwxrwxr-x. 3 train train        96 Aug 21 06:19 churn-telecom\r\n",
      "-rw-rw-r--. 1 train train  41002480 Aug 21 06:18 Fire_Incidents.csv.gz\r\n",
      "drwxrwxr-x. 7 train train        67 Aug 21 09:09 flight-data\r\n",
      "-rw-rw-r--. 1 train train  46401315 Aug 21 06:18 Hotel_Reviews.csv.gz\r\n",
      "-rw-rw-r--. 1 train train  44525776 Aug 21 06:17 market1mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Aug 21 12:13 market1mil_snappyparquet\r\n",
      "-rw-rw-r--. 1 train train 269015852 Aug 21 06:18 market5mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train         6 Aug 21 09:37 market5mil_lzoparquet\r\n",
      "drwxrwxr-x. 2 train train       198 Aug 21 12:11 market5mil_snappyparquet\r\n",
      "drwxrwxr-x. 2 train train       133 Aug 21 06:18 retail_db\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "SparkSession.builder.appName(\"Joins\").master(\"local[2]\")\n",
    "    .config(\"spark.executor.memory\",\"3g\")\n",
    "    .config(\"spark.driver.memory\",\"512m\")\n",
    "    .config(\"spark.memory.fraction\",\"0.1\")\n",
    "    .config(\"spark.memory.storageFraction\",\"0.0\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"file:///home/train/datasets/retail_db/categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+\n",
      "|categoryId|categoryDepartmentId|       categoryName|\n",
      "+----------+--------------------+-------------------+\n",
      "|         1|                   2|           Football|\n",
      "|         2|                   2|             Soccer|\n",
      "|         3|                   2|Baseball & Softball|\n",
      "+----------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"file:///home/train/datasets/retail_db/departments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|departmentId|departmentName|\n",
      "+------------+--------------+\n",
      "|           2|       Fitness|\n",
      "|           3|      Footwear|\n",
      "|           4|       Apparel|\n",
      "|           5|          Golf|\n",
      "|           6|      Outdoors|\n",
      "|           7|      Fan Shop|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcast_join_df = categories.join(F.broadcast(departments), \n",
    "                                categories.categoryDepartmentId == departments.departmentId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+------------+--------------+\n",
      "|categoryId|categoryDepartmentId|       categoryName|departmentId|departmentName|\n",
      "+----------+--------------------+-------------------+------------+--------------+\n",
      "|         1|                   2|           Football|           2|       Fitness|\n",
      "|         2|                   2|             Soccer|           2|       Fitness|\n",
      "|         3|                   2|Baseball & Softball|           2|       Fitness|\n",
      "|         4|                   2|         Basketball|           2|       Fitness|\n",
      "|         5|                   2|           Lacrosse|           2|       Fitness|\n",
      "|         6|                   2|   Tennis & Racquet|           2|       Fitness|\n",
      "|         7|                   2|             Hockey|           2|       Fitness|\n",
      "|         8|                   2|        More Sports|           2|       Fitness|\n",
      "|         9|                   3|   Cardio Equipment|           3|      Footwear|\n",
      "|        10|                   3|  Strength Training|           3|      Footwear|\n",
      "|        11|                   3|Fitness Accessories|           3|      Footwear|\n",
      "|        12|                   3|       Boxing & MMA|           3|      Footwear|\n",
      "|        13|                   3|        Electronics|           3|      Footwear|\n",
      "|        14|                   3|     Yoga & Pilates|           3|      Footwear|\n",
      "|        15|                   3|  Training by Sport|           3|      Footwear|\n",
      "|        16|                   3|    As Seen on  TV!|           3|      Footwear|\n",
      "|        17|                   4|             Cleats|           4|       Apparel|\n",
      "|        18|                   4|     Men's Footwear|           4|       Apparel|\n",
      "|        19|                   4|   Women's Footwear|           4|       Apparel|\n",
      "|        20|                   4|     Kids' Footwear|           4|       Apparel|\n",
      "+----------+--------------------+-------------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bcast_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The BHJ is the easiest and fastest join Spark offers, since it does not involve any shuffle\n",
    "of the data set; all the data is available locally to the executor after a broadcast. You\n",
    "just have to be sure that you have enough memory both on the Spark driver’s and the\n",
    "executors’ side to hold the smaller data set in memory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [categoryDepartmentId#17], [departmentId#54], Inner, BuildRight\n",
      ":- *(2) Project [categoryId#16, categoryDepartmentId#17, categoryName#18]\n",
      ":  +- *(2) Filter isnotnull(categoryDepartmentId#17)\n",
      ":     +- FileScan csv [categoryId#16,categoryDepartmentId#17,categoryName#18] Batched: false, DataFilters: [isnotnull(categoryDepartmentId#17)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/categories.csv], PartitionFilters: [], PushedFilters: [IsNotNull(categoryDepartmentId)], ReadSchema: struct<categoryId:int,categoryDepartmentId:int,categoryName:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#133]\n",
      "   +- *(1) Project [departmentId#54, departmentName#55]\n",
      "      +- *(1) Filter isnotnull(departmentId#54)\n",
      "         +- FileScan csv [departmentId#54,departmentName#55] Batched: false, DataFilters: [isnotnull(departmentId#54)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/departments.csv], PartitionFilters: [], PushedFilters: [IsNotNull(departmentId)], ReadSchema: struct<departmentId:int,departmentName:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Join Inner, (categoryDepartmentId#17 = departmentId#54), rightHint=(strategy=broadcast), Statistics(sizeInBytes=105.1 KiB)\n",
      ":- Filter isnotnull(categoryDepartmentId#17), Statistics(sizeInBytes=1133.0 B)\n",
      ":  +- Relation[categoryId#16,categoryDepartmentId#17,categoryName#18] csv, Statistics(sizeInBytes=1133.0 B)\n",
      "+- Filter isnotnull(departmentId#54), Statistics(sizeInBytes=95.0 B)\n",
      "   +- Relation[departmentId#54,departmentName#55] csv, Statistics(sizeInBytes=95.0 B)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) BroadcastHashJoin [categoryDepartmentId#17], [departmentId#54], Inner, BuildRight\n",
      ":- *(2) Project [categoryId#16, categoryDepartmentId#17, categoryName#18]\n",
      ":  +- *(2) Filter isnotnull(categoryDepartmentId#17)\n",
      ":     +- FileScan csv [categoryId#16,categoryDepartmentId#17,categoryName#18] Batched: false, DataFilters: [isnotnull(categoryDepartmentId#17)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/categories.csv], PartitionFilters: [], PushedFilters: [IsNotNull(categoryDepartmentId)], ReadSchema: struct<categoryId:int,categoryDepartmentId:int,categoryName:string>\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint))), [id=#133]\n",
      "   +- *(1) Project [departmentId#54, departmentName#55]\n",
      "      +- *(1) Filter isnotnull(departmentId#54)\n",
      "         +- FileScan csv [departmentId#54,departmentName#55] Batched: false, DataFilters: [isnotnull(departmentId#54)], Format: CSV, Location: InMemoryFileIndex[file:/home/train/venvspark/dev/data/retail_db/departments.csv], PartitionFilters: [], PushedFilters: [IsNotNull(departmentId)], ReadSchema: struct<departmentId:int,departmentName:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bcast_join_df.explain('cost')\n",
    "# The modes include 'simple', 'extended', 'codegen', 'cost', and 'formatted'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Use this type of join under the following conditions for maximum benefit:\n",
    "    • When each key within the smaller and larger data sets is hashed to the same partition\n",
    "    by Spark\n",
    "    • When one data set is much smaller than the other (and within the default config\n",
    "    of 10 MB, or more if you have sufficient memory)\n",
    "    • When you only want to perform an equi-join, to combine two data sets based on\n",
    "    matching unsorted keys\n",
    "    • When you are not worried by excessive network bandwidth usage or OOM\n",
    "    errors, because the smaller data set will be broadcast to all Spark executors\n",
    "    Specifying a value of -1 in spark.sql.autoBroadcastJoinThreshold will cause\n",
    "    Spark to always resort to a shuffle sort merge join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
