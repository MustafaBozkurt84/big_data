{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/train/output_data/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -R -skipTrash /user/train/output_data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls  /user/train/output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding external libs to application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"Dataframe Reader\") \\\n",
    ".master(\"yarn\") \\\n",
    ".enableHiveSupport() \\\n",
    ".config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.12:3.0.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.12:3.0.0\") \\\n",
    "# is for avro. Avro is not bulit-in data format\n",
    "# <groupId>:<artifactId>:<version>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! hdfs dfs -put /home/train/datasets/flight-data  /user/train/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\r\n",
      "-rw-r--r--   1 train supergroup       4556 2020-11-15 10:29 /user/train/datasets/Advertising.csv\r\n",
      "-rw-r--r--   1 train supergroup   46401315 2020-11-14 16:17 /user/train/datasets/Hotel_Reviews.csv.gz\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-10 22:03 /user/train/datasets/cat_images\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-15 09:55 /user/train/datasets/flight-data\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-02 21:45 /user/train/datasets/hiveExternal\r\n",
      "-rw-r--r--   1 train supergroup       4611 2020-11-01 12:21 /user/train/datasets/iris.csv\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-15 08:34 /user/train/datasets/market1mil_snappyparquet\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-15 08:30 /user/train/datasets/market5mil_snappyparquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/train/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrameReader methods, arguments, and options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_dataframe_reader_table.png\"/>\n",
    "\n",
    "<p>Source: Learning Spark, O'Reilly, 2020</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"hdfs://localhost:9000/user/train/datasets/flight-data/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't have to point a single file with .csv extension.\n",
    "# You can read from a folder.\n",
    "# But all csv files have to be same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>India</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
       "0      United States             Romania      1\n",
       "1      United States             Ireland    264\n",
       "2      United States               India     69\n",
       "3              Egypt       United States     24\n",
       "4  Equatorial Guinea       United States      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_schema = \"DEST_COUNTRY_NAME string, ORIGIN_COUNTRY_NAME string, count integer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read with pre-defined schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", True) \\\n",
    ".schema(flight_schema) \\\n",
    ".option(\"sep\", \",\") \\\n",
    ".load(\"hdfs://localhost:9000/user/train/datasets/flight-data/csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>India</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
       "0      United States             Romania      1\n",
       "1      United States             Ireland    264\n",
       "2      United States               India     69\n",
       "3              Egypt       United States     24\n",
       "4  Equatorial Guinea       United States      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Parquet is the default and preferred data source for Spark because\n",
    "    it’s efficient, uses columnar storage, and employs a fast compression\n",
    "    algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read \\\n",
    ".format(\"parquet\") \\\n",
    ".load(\"hdfs://localhost:9000/user/train/datasets/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>India</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
       "0      United States             Romania      1\n",
       "1      United States             Ireland    264\n",
       "2      United States               India     69\n",
       "3              Egypt       United States     24\n",
       "4  Equatorial Guinea       United States      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! hdfs dfs -put /home/train/datasets/cat_images/ /user/train/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = spark.read.format(\"image\") \\\n",
    ".load(\"hdfs://localhost:9000/user/train/datasets/cat_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+------+-----+---------+----+\n",
      "|origin                                                         |height|width|nChannels|mode|\n",
      "+---------------------------------------------------------------+------+-----+---------+----+\n",
      "|hdfs://localhost:9000/user/train/datasets/cat_images/cat.3.jpg |414   |500  |3        |16  |\n",
      "|hdfs://localhost:9000/user/train/datasets/cat_images/cat.7.jpg |499   |495  |3        |16  |\n",
      "|hdfs://localhost:9000/user/train/datasets/cat_images/cat.10.jpg|499   |489  |3        |16  |\n",
      "|hdfs://localhost:9000/user/train/datasets/cat_images/cat.17.jpg|375   |499  |3        |16  |\n",
      "|hdfs://localhost:9000/user/train/datasets/cat_images/cat.18.jpg|374   |500  |3        |16  |\n",
      "+---------------------------------------------------------------+------+-----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_df.select(\"image.origin\", \"image.height\", \"image.width\", \"image.nChannels\",\"image.mode\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Spark 3.0 adds support for binary files as a data source. The DataFrameReader converts\n",
    "    each binary file into a single DataFrame row (record) that contains the raw content\n",
    "    and metadata of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_files_df = (spark.read\n",
    ".format(\"binaryFile\")\n",
    ".option(\"pathGlobFilter\", \"*.jpg\")\n",
    ".option(\"recursiveFile\", True)\n",
    ".load(\"hdfs://localhost:9000/user/train/datasets/cat_images\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+\n",
      "|                path|    modificationTime|length|             content|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "|hdfs://localhost:...|2020-11-10 22:03:...| 37971|[FF D8 FF E0 00 1...|\n",
      "|hdfs://localhost:...|2020-11-10 22:03:...| 36934|[FF D8 FF E0 00 1...|\n",
      "|hdfs://localhost:...|2020-11-10 22:03:...| 34315|[FF D8 FF E0 00 1...|\n",
      "|hdfs://localhost:...|2020-11-10 22:03:...| 32072|[FF D8 FF E0 00 1...|\n",
      "|hdfs://localhost:...|2020-11-10 22:03:...| 30119|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataframeWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    DataFrameWriter.format(args)\n",
    "    .option(args)\n",
    "    .bucketBy(args)\n",
    "    .partitionBy(args)\n",
    "    .save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_dataframe_writer_table.png\"/>\n",
    "\n",
    "<p>Source: Learning Spark, O'Reilly, 2020</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet is the default and recommended format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.5220863819122314 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"parquet\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/parquet\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 K   13.4 K   hdfs://localhost:9000/user/train/output_data/flight-data/avro\r\n",
      "40.6 K   40.6 K   hdfs://localhost:9000/user/train/output_data/flight-data/csv\r\n",
      "122.8 K  122.8 K  hdfs://localhost:9000/user/train/output_data/flight-data/json\r\n",
      "11.2 K   11.2 K   hdfs://localhost:9000/user/train/output_data/flight-data/orc\r\n",
      "14.7 K   14.7 K   hdfs://localhost:9000/user/train/output_data/flight-data/parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h hdfs://localhost:9000/user/train/output_data/flight-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n_SUCCESS\\n_committed_1799640464332036264\\n_started_1799640464332036264\\npart-00000-tid-1799640464332036264-91273258-d7ef-4dc7-<...>-c000.snappy.parquet\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will create a folder and inside will be\n",
    "\"\"\"\n",
    "_SUCCESS\n",
    "_committed_1799640464332036264\n",
    "_started_1799640464332036264\n",
    "part-00000-tid-1799640464332036264-91273258-d7ef-4dc7-<...>-c000.snappy.parquet\n",
    "\"\"\"\n",
    "# part-xxxx...  can be more than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.413262128829956 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"parquet\") \\\n",
    ".option(\"compression\", \"snappy\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/parquet\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.7 K  14.7 K  hdfs://localhost:9000/user/train/output_data/flight-data/parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h hdfs://localhost:9000/user/train/output_data/flight-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "market5 = spark.read.format(\"csv\") \\\n",
    ".option(\"compression\",\"gzip\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \"\\t\") \\\n",
    ".load(\"file:///home/train/datasets/market5mil.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5387992"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOGICALREF</th>\n",
       "      <th>COUNT_</th>\n",
       "      <th>ITEMCODE</th>\n",
       "      <th>ITEMNAME</th>\n",
       "      <th>FICHENO</th>\n",
       "      <th>DATE_</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>LINENETTOTAL</th>\n",
       "      <th>LINENET</th>\n",
       "      <th>...</th>\n",
       "      <th>CLIENTNAME</th>\n",
       "      <th>BRANDCODE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>CATEGORY_NAME1</th>\n",
       "      <th>CATEGORY_NAME2</th>\n",
       "      <th>CATEGORY_NAME3</th>\n",
       "      <th>STARTDATE</th>\n",
       "      <th>ENDDATE</th>\n",
       "      <th>SPECODE</th>\n",
       "      <th>CAPIBLOCK_CREADEDDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>00000000008</td>\n",
       "      <td>TOZ SEKER</td>\n",
       "      <td>0000000000015560</td>\n",
       "      <td>2017-01-02 00:00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>2,6499999999999999</td>\n",
       "      <td>5,2999999999999998</td>\n",
       "      <td>4,9000000000000004</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>İÇECEK</td>\n",
       "      <td>ÇAY KAHVE</td>\n",
       "      <td>SEKER TATLANDIRICI</td>\n",
       "      <td>2017-01-03 09:25:03</td>\n",
       "      <td>2017-01-03 09:25:43</td>\n",
       "      <td>K</td>\n",
       "      <td>2018-07-14 01:50:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>00000020868</td>\n",
       "      <td>KIRMIZI MERCIMEK</td>\n",
       "      <td>0000000000015560</td>\n",
       "      <td>2017-01-02 00:00:00</td>\n",
       "      <td>1,006</td>\n",
       "      <td>2,7999999999999998</td>\n",
       "      <td>2,8199999999999998</td>\n",
       "      <td>2,79</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>167</td>\n",
       "      <td>BAKLİYAT</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>BAKLİYAT</td>\n",
       "      <td>AÇIK BAKLİYAT</td>\n",
       "      <td>2017-01-03 09:25:03</td>\n",
       "      <td>2017-01-03 09:25:43</td>\n",
       "      <td>K</td>\n",
       "      <td>2018-07-14 01:50:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>00000008583</td>\n",
       "      <td>TEST MATIK 1,5 KG NORMAL</td>\n",
       "      <td>0000000000015560</td>\n",
       "      <td>2017-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4,9500000000000002</td>\n",
       "      <td>4,9500000000000002</td>\n",
       "      <td>4,1900000000000004</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>229</td>\n",
       "      <td>TEST</td>\n",
       "      <td>DETERJAN TEMİZLİK</td>\n",
       "      <td>ÇAMAŞIR YIKAMA</td>\n",
       "      <td>TOZ DETERJAN</td>\n",
       "      <td>2017-01-03 09:25:03</td>\n",
       "      <td>2017-01-03 09:25:43</td>\n",
       "      <td>K</td>\n",
       "      <td>2018-07-14 01:50:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>00000001454</td>\n",
       "      <td>BIZIM MAKARNA BONCUK</td>\n",
       "      <td>0000000000015560</td>\n",
       "      <td>2017-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1000000000000001</td>\n",
       "      <td>1,1000000000000001</td>\n",
       "      <td>1,02</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>146</td>\n",
       "      <td>ÜLKER</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>2017-01-03 09:25:03</td>\n",
       "      <td>2017-01-03 09:25:43</td>\n",
       "      <td>K</td>\n",
       "      <td>2018-07-14 01:50:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>00000013519</td>\n",
       "      <td>FILIZ MAKARNA KISA KESME 500 GR</td>\n",
       "      <td>0000000000015560</td>\n",
       "      <td>2017-01-02 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1000000000000001</td>\n",
       "      <td>1,1000000000000001</td>\n",
       "      <td>1,02</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>52</td>\n",
       "      <td>FİLİZ</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>2017-01-03 09:25:03</td>\n",
       "      <td>2017-01-03 09:25:43</td>\n",
       "      <td>K</td>\n",
       "      <td>2018-07-14 01:50:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LOGICALREF  COUNT_     ITEMCODE                         ITEMNAME  \\\n",
       "0           1       1  00000000008                        TOZ SEKER   \n",
       "1           2       1  00000020868                 KIRMIZI MERCIMEK   \n",
       "2           3       1  00000008583         TEST MATIK 1,5 KG NORMAL   \n",
       "3           4       1  00000001454             BIZIM MAKARNA BONCUK   \n",
       "4           5       1  00000013519  FILIZ MAKARNA KISA KESME 500 GR   \n",
       "\n",
       "            FICHENO                DATE_ AMOUNT               PRICE  \\\n",
       "0  0000000000015560  2017-01-02 00:00:00     45  2,6499999999999999   \n",
       "1  0000000000015560  2017-01-02 00:00:00  1,006  2,7999999999999998   \n",
       "2  0000000000015560  2017-01-02 00:00:00      1  4,9500000000000002   \n",
       "3  0000000000015560  2017-01-02 00:00:00      1  1,1000000000000001   \n",
       "4  0000000000015560  2017-01-02 00:00:00      1  1,1000000000000001   \n",
       "\n",
       "         LINENETTOTAL             LINENET  ...      CLIENTNAME BRANDCODE  \\\n",
       "0  5,2999999999999998  4,9000000000000004  ...  Hanım CANBULAT      None   \n",
       "1  2,8199999999999998                2,79  ...  Hanım CANBULAT       167   \n",
       "2  4,9500000000000002  4,1900000000000004  ...  Hanım CANBULAT       229   \n",
       "3  1,1000000000000001                1,02  ...  Hanım CANBULAT       146   \n",
       "4  1,1000000000000001                1,02  ...  Hanım CANBULAT        52   \n",
       "\n",
       "      BRAND     CATEGORY_NAME1  CATEGORY_NAME2      CATEGORY_NAME3  \\\n",
       "0      None             İÇECEK       ÇAY KAHVE  SEKER TATLANDIRICI   \n",
       "1  BAKLİYAT               GIDA        BAKLİYAT       AÇIK BAKLİYAT   \n",
       "2      TEST  DETERJAN TEMİZLİK  ÇAMAŞIR YIKAMA        TOZ DETERJAN   \n",
       "3     ÜLKER               GIDA         MAKARNA            MAKARNA    \n",
       "4     FİLİZ               GIDA         MAKARNA            MAKARNA    \n",
       "\n",
       "             STARTDATE              ENDDATE SPECODE CAPIBLOCK_CREADEDDATE  \n",
       "0  2017-01-03 09:25:03  2017-01-03 09:25:43       K   2018-07-14 01:50:39  \n",
       "1  2017-01-03 09:25:03  2017-01-03 09:25:43       K   2018-07-14 01:50:39  \n",
       "2  2017-01-03 09:25:03  2017-01-03 09:25:43       K   2018-07-14 01:50:39  \n",
       "3  2017-01-03 09:25:03  2017-01-03 09:25:43       K   2018-07-14 01:50:39  \n",
       "4  2017-01-03 09:25:03  2017-01-03 09:25:43       K   2018-07-14 01:50:39  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market5.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">!!!!!!   Caution: If you use existing folder, the data inside folder will be deleted !!!</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ead6ebd10fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmarket5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"snappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://localhost:9000/user/train/datasets/market5mil_snappyparquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Available codecs are brotli, uncompressed, lz4, gzip, lzo, snappy, none, zstd.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/manual/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/manual/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/manual/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/manual/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "market5.write.mode(\"overwrite\") \\\n",
    ".option(\"compression\", \"snappy\") \\\n",
    ".parquet(\"hdfs://localhost:9000/user/train/datasets/market5mil_snappyparquet\")\n",
    "\n",
    "# Available codecs are brotli, uncompressed, lz4, gzip, lzo, snappy, none, zstd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - train supergroup          0 2020-11-15 08:30 /user/train/datasets/market5mil_snappyparquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/train/datasets | grep market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "market1 = spark.read.format(\"csv\") \\\n",
    ".option(\"compression\",\"gzip\") \\\n",
    ".option(\"header\", True) \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".option(\"sep\", \";\") \\\n",
    ".load(\"file:///home/train/datasets/market1mil.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOGICALREF</th>\n",
       "      <th>COUNT_</th>\n",
       "      <th>ITEMCODE</th>\n",
       "      <th>ITEMNAME</th>\n",
       "      <th>FICHENO</th>\n",
       "      <th>DATE_</th>\n",
       "      <th>AMOUNT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>LINENETTOTAL</th>\n",
       "      <th>LINENET</th>\n",
       "      <th>...</th>\n",
       "      <th>CLIENTNAME</th>\n",
       "      <th>BRANDCODE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>CATEGORY_NAME1</th>\n",
       "      <th>CATEGORY_NAME2</th>\n",
       "      <th>CATEGORY_NAME3</th>\n",
       "      <th>STARTDATE</th>\n",
       "      <th>ENDDATE</th>\n",
       "      <th>SPECODE</th>\n",
       "      <th>CAPIBLOCK_CREADEDDATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>TOZ SEKER</td>\n",
       "      <td>15560</td>\n",
       "      <td>2.01.2017 00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>2,65</td>\n",
       "      <td>5,3</td>\n",
       "      <td>4,9</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>İÇECEK</td>\n",
       "      <td>ÇAY KAHVE</td>\n",
       "      <td>SEKER TATLANDIRICI</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>K</td>\n",
       "      <td>14.07.2018 01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20868</td>\n",
       "      <td>KIRMIZI MERCIMEK</td>\n",
       "      <td>15560</td>\n",
       "      <td>2.01.2017 00:00</td>\n",
       "      <td>1,006</td>\n",
       "      <td>2,8</td>\n",
       "      <td>2,82</td>\n",
       "      <td>2,79</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>167</td>\n",
       "      <td>BAKLİYAT</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>BAKLİYAT</td>\n",
       "      <td>AÇIK BAKLİYAT</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>K</td>\n",
       "      <td>14.07.2018 01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8583</td>\n",
       "      <td>TEST MATIK 1,5 KG NORMAL</td>\n",
       "      <td>15560</td>\n",
       "      <td>2.01.2017 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>4,95</td>\n",
       "      <td>4,95</td>\n",
       "      <td>4,19</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>229</td>\n",
       "      <td>TEST</td>\n",
       "      <td>DETERJAN TEMİZLİK</td>\n",
       "      <td>ÇAMAŞIR YIKAMA</td>\n",
       "      <td>TOZ DETERJAN</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>K</td>\n",
       "      <td>14.07.2018 01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1454</td>\n",
       "      <td>BIZIM MAKARNA BONCUK</td>\n",
       "      <td>15560</td>\n",
       "      <td>2.01.2017 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1</td>\n",
       "      <td>1,1</td>\n",
       "      <td>1,02</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>146</td>\n",
       "      <td>ÜLKER</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>K</td>\n",
       "      <td>14.07.2018 01:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13519</td>\n",
       "      <td>FILIZ MAKARNA KISA KESME 500 GR</td>\n",
       "      <td>15560</td>\n",
       "      <td>2.01.2017 00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1,1</td>\n",
       "      <td>1,1</td>\n",
       "      <td>1,02</td>\n",
       "      <td>...</td>\n",
       "      <td>Hanım CANBULAT</td>\n",
       "      <td>52</td>\n",
       "      <td>FİLİZ</td>\n",
       "      <td>GIDA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>MAKARNA</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>3.01.2017 09:25</td>\n",
       "      <td>K</td>\n",
       "      <td>14.07.2018 01:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOGICALREF  COUNT_  ITEMCODE                         ITEMNAME  FICHENO  \\\n",
       "0          1       1         8                        TOZ SEKER    15560   \n",
       "1          2       1     20868                 KIRMIZI MERCIMEK    15560   \n",
       "2          3       1      8583         TEST MATIK 1,5 KG NORMAL    15560   \n",
       "3          4       1      1454             BIZIM MAKARNA BONCUK    15560   \n",
       "4          5       1     13519  FILIZ MAKARNA KISA KESME 500 GR    15560   \n",
       "\n",
       "             DATE_ AMOUNT PRICE LINENETTOTAL LINENET  ...      CLIENTNAME  \\\n",
       "0  2.01.2017 00:00     45  2,65          5,3     4,9  ...  Hanım CANBULAT   \n",
       "1  2.01.2017 00:00  1,006   2,8         2,82    2,79  ...  Hanım CANBULAT   \n",
       "2  2.01.2017 00:00      1  4,95         4,95    4,19  ...  Hanım CANBULAT   \n",
       "3  2.01.2017 00:00      1   1,1          1,1    1,02  ...  Hanım CANBULAT   \n",
       "4  2.01.2017 00:00      1   1,1          1,1    1,02  ...  Hanım CANBULAT   \n",
       "\n",
       "  BRANDCODE     BRAND     CATEGORY_NAME1  CATEGORY_NAME2      CATEGORY_NAME3  \\\n",
       "0      None      None             İÇECEK       ÇAY KAHVE  SEKER TATLANDIRICI   \n",
       "1       167  BAKLİYAT               GIDA        BAKLİYAT       AÇIK BAKLİYAT   \n",
       "2       229      TEST  DETERJAN TEMİZLİK  ÇAMAŞIR YIKAMA        TOZ DETERJAN   \n",
       "3       146     ÜLKER               GIDA         MAKARNA            MAKARNA    \n",
       "4        52     FİLİZ               GIDA         MAKARNA            MAKARNA    \n",
       "\n",
       "         STARTDATE          ENDDATE SPECODE CAPIBLOCK_CREADEDDATE  \n",
       "0  3.01.2017 09:25  3.01.2017 09:25       K      14.07.2018 01:50  \n",
       "1  3.01.2017 09:25  3.01.2017 09:25       K      14.07.2018 01:50  \n",
       "2  3.01.2017 09:25  3.01.2017 09:25       K      14.07.2018 01:50  \n",
       "3  3.01.2017 09:25  3.01.2017 09:25       K      14.07.2018 01:50  \n",
       "4  3.01.2017 09:25  3.01.2017 09:25       K      14.07.2018 01:50  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "market1.write.mode(\"overwrite\") \\\n",
    ".option(\"compression\", \"snappy\") \\\n",
    ".parquet(\"hdfs://localhost:9000/user/train/datasets/market1mil_snappyparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - train supergroup          0 2020-11-15 08:34 /user/train/datasets/market1mil_snappyparquet\r\n",
      "drwxr-xr-x   - train supergroup          0 2020-11-15 08:30 /user/train/datasets/market5mil_snappyparquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/train/datasets | grep market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.2660014629364014 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/orc\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.9012176990509033 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"json\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/json\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.4392178058624268 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"csv\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/csv\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Avro format is used, for example,\n",
    "    by Apache Kafka for message serializing and deserializing. It offers many benefits,\n",
    "    including direct mapping to JSON, speed and efficiency, and bindings available\n",
    "    for many programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.7004945278167725 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_csv.write \\\n",
    ".format(\"avro\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"hdfs://localhost:9000/user/train/output_data/flight-data/avro\")\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine output sizes for each format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 K   13.4 K   hdfs://localhost:9000/user/train/output_data/flight-data/avro\r\n",
      "40.6 K   40.6 K   hdfs://localhost:9000/user/train/output_data/flight-data/csv\r\n",
      "122.8 K  122.8 K  hdfs://localhost:9000/user/train/output_data/flight-data/json\r\n",
      "11.2 K   11.2 K   hdfs://localhost:9000/user/train/output_data/flight-data/orc\r\n",
      "14.7 K   14.7 K   hdfs://localhost:9000/user/train/output_data/flight-data/parquet\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -du -h hdfs://localhost:9000/user/train/output_data/flight-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we don't use enableHiveSupport() during the SparkSession\n",
    "# following code will list spark catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "| bookstore|\n",
      "|   default|\n",
      "|homecredit|\n",
      "|    retail|\n",
      "|     test1|\n",
      "|     test2|\n",
      "|     train|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create database if not exists train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.write \\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".saveAsTable(\"train.flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "|   train|       flights|      false|\n",
      "|   train|sum_of_flights|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use train\")\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from train.flights limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
