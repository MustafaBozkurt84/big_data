{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Missing Data\n",
    "\n",
    "Being able to plot missing values is a great way to quickly understand how much of your data is missing. It can also help highlight when variables are missing in a pattern something that will need to be handled with care lest your model be biased.\n",
    "\n",
    "Which variable has the most missing values? Run all lines of code except the last one to determine the answer. Once you're confident, and fill out the value and hit \"Submit Answer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataframe and convert to Pandas\n",
    "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
    "sample_df = sample_df.toPandas()\n",
    "\n",
    "# Convert all values to T/F\n",
    "tf_df = sample_df.isnull()\n",
    "\n",
    "# Plot it\n",
    "sns.heatmap(data=tf_df)\n",
    "plt.xticks(rotation=30, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Set the answer to the column with the most missing data\n",
    "answer = 'BACKONMARKETDATE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Data\n",
    "\n",
    "Missing data happens. If we make the assumption that our data is missing completely at random, we are making the assumption that what data we do have, is a good representation of the population. If we have a few values we could remove them or we could use the mean or median as a replacement. In this exercise, we will look at 'PDOM': Days on Market at Current Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows\n",
    "missing = df.where(df['PDOM'].isNull()).count()\n",
    "\n",
    "# Calculate the mean value\n",
    "col_mean = df.agg({'PDOM': \"mean\"}).collect()[0][0]\n",
    "\n",
    "# Replacing with the mean value for that column\n",
    "df.fillna(col_mean, subset=['PDOM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Missing Percents\n",
    "\n",
    "Automation is the future of data science. Learning to automate some of your data preparation pays dividends. In this exercise, we will automate dropping columns if they are missing data beyond a specific threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_dropper(df, threshold):\n",
    "  # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
    "  total_records = df.count()\n",
    "  for col in df.columns:\n",
    "    # Calculate the percentage of missing values\n",
    "    missing = df.where(df[col].isNull()).count()\n",
    "    missing_percent = missing / total_records\n",
    "    # Drop column if percent of missing is more than threshold\n",
    "    if missing_percent > threshold:\n",
    "      df = df.drop(col)\n",
    "  return df\n",
    "\n",
    "# Drop columns that are more than 60% missing\n",
    "df = column_dropper(df, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Dangerous Join\n",
    "\n",
    "In this exercise, we will be joining on Latitude and Longitude to bring in another dataset that measures how walk-friendly a neighborhood is. We'll need to be careful to make sure our joining columns are the same data type and ensure we are joining on the same precision (number of digits after the decimal) or our join won't work!\n",
    "\n",
    "Below you will find that df['latitude'] and df['longitude'] are at a higher precision than walk_df['longitude'] and walk_df['latitude'] we'll need to round them to the same precision so the join will work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data types\n",
    "walk_df = walk_df.withColumn('longitude', walk_df[\"longitude\"].cast('double'))\n",
    "walk_df = walk_df.withColumn(\"latitude\", walk_df[\"latitude\"].cast('double'))\n",
    "\n",
    "# Round precision\n",
    "df = df.withColumn('longitude', round(df[\"longitude\"], 5))\n",
    "df = df.withColumn(\"latitude\", round(df[\"latitude\"], 5))\n",
    "\n",
    "# Create join condition\n",
    "condition = [walk_df[\"latitude\"] == df[\"latitude\"], walk_df[\"longitude\"] ==df[\"longitude\"]]\n",
    "\n",
    "# Join the dataframes together\n",
    "join_df = walk_df.join(df, on=condition, how=\"left\")\n",
    "# Count non-null records from new field\n",
    "print(join_df.where(~join_df['walkscore'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL Join\n",
    "\n",
    "Sometimes it is much easier to write complex joins in SQL. In this exercise, we will start with the join keys already in the same format and precision but will use SparkSQL to do the joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register dataframes as tables\n",
    "walk_df.createOrReplaceTempView(\"walk_df\")\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# SQL to join dataframes\n",
    "join_sql = \t\"\"\"\n",
    "\t\t\tSELECT \n",
    "\t\t\t\t*\n",
    "\t\t\tFROM df\n",
    "\t\t\tLEFT JOIN walk_df\n",
    "\t\t\tON df.longitude = walk_df.longitude\n",
    "\t\t\tAND df.latitude = walk_df.latitude\n",
    "\t\t\t\"\"\"\n",
    "# Perform sql join\n",
    "joined_df = spark.sql(join_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Bad Joins\n",
    "\n",
    "Joins can go bad silently if we are not careful, meaning they will not error out but instead return mangled data with more or less data than you'd intended. Let's take a look at a couple ways that joining incorrectly can change your data set for the worse.\n",
    "\n",
    "In this example we will look at what happens if you join two dataframes together when the join keys are not the same precision and compare the record counts between the correct join and the incorrect one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on mismatched keys precision \n",
    "wrong_prec_cond = [df_orig.longitude == walk_df.longitude, df_orig.latitude == walk_df.latitude]\n",
    "wrong_prec_df = df_orig.join(walk_df, on=wrong_prec_cond, how='left')\n",
    "\n",
    "# Compare bad join to the correct one\n",
    "print(wrong_prec_df.where(wrong_prec_df[\"walkscore\"].isNull()).count())\n",
    "print(correct_join_df.where(correct_join_df[\"walkscore\"].isNull()).count())\n",
    "\n",
    "# Create a join on too few keys\n",
    "few_keys_cond = [df[\"longitude\"] == walk_df[\"longitude\"]]\n",
    "few_keys_df = df.join(walk_df, on=few_keys_cond, how='left')\n",
    "\n",
    "# Compare bad join to the correct one\n",
    "print(\"Record Count of the Too Few Keys Join Example: \" + str(few_keys_df.count()))\n",
    "print(\"Record Count of the Correct Join Example: \" + str(correct_join_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences -- generating features\n",
    "\n",
    "Let's explore generating features using existing ones. In the midwest of the U.S. many single family homes have extra land around them for green space. In this example you will create a new feature called 'YARD_SIZE', and then see if the new feature is correlated with our outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot size in square feet\n",
    "acres_to_sqfeet = 43560\n",
    "df = df.withColumn(\"LOT_SIZE_SQFT\", df[\"ACRES\"] * acres_to_sqfeet)\n",
    "\n",
    "# Create new column YARD_SIZE\n",
    "df = df.withColumn(\"YARD_SIZE\", df[\"LOT_SIZE_SQFT\"] - df[\"FOUNDATIONSIZE\"])\n",
    "\n",
    "# Corr of ACRES vs SALESCLOSEPRICE\n",
    "print(\"Corr of ACRES vs SALESCLOSEPRICE: \" + str(df.corr(\"ACRES\", \"SALESCLOSEPRICE\")))\n",
    "# Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE: \" + str(df.corr(\"FOUNDATIONSIZE\", \"SALESCLOSEPRICE\")))\n",
    "# Corr of YARD_SIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of YARD_SIZE vs SALESCLOSEPRICE: \" + str(df.corr(\"YARD_SIZE\", \"SALESCLOSEPRICE\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratios\n",
    "\n",
    "Ratios are all around us. Whether it's miles per gallon or click through rate, they are everywhere. In this exercise, we'll create some ratios by dividing out pairs of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSED_TO_LIST\n",
    "df = df.withColumn(\"ASSESSED_TO_LIST\",df[\"ASSESSEDVALUATION\"]/df[\"LISTPRICE\"])\n",
    "df[['ASSESSEDVALUATION', 'LISTPRICE', 'ASSESSED_TO_LIST']].show(5)\n",
    "# TAX_TO_LIST\n",
    "df = df.withColumn(\"TAX_TO_LIST\",df[\"TAXES\"]/df[\"LISTPRICE\"])\n",
    "df[['TAX_TO_LIST', 'TAXES', 'LISTPRICE']].show(5)\n",
    "# BED_TO_BATHS\n",
    "df = df.withColumn(\"BED_TO_BATHS\",df[\"BEDROOMS\"]/df[\"BATHSTOTAL\"])\n",
    "df[['BED_TO_BATHS', 'BEDROOMS', 'BATHSTOTAL']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Features\n",
    "\n",
    "In previous exercises we showed how combining two features together can create good additional features for a predictive model. In this exercise, you will generate 'deeper' features by combining the effects of three variables into one. Then you will check to see if deeper and more complicated features always make for better predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature by adding two features together\n",
    "df = df.withColumn('Total_SQFT', df['SQFTBELOWGROUND'] + df['SQFTABOVEGROUND'])\n",
    "\n",
    "# Create additional new feature using previously created feature\n",
    "df = df.withColumn('BATHS_PER_1000SQFT', df['BATHSTOTAL'] / (df['Total_SQFT'] / 1000))\n",
    "df[['BATHS_PER_1000SQFT']].describe().show()\n",
    "\n",
    "# Pandas dataframe\n",
    "pandas_df = df.sample(False, 0.5, 0).toPandas()\n",
    "\n",
    "# Linear model plots\n",
    "sns.jointplot(x='Total_SQFT', y='SALESCLOSEPRICE', data=pandas_df, kind=\"reg\", stat_func=r2)\n",
    "plt.show()\n",
    "sns.jointplot(x='BATHS_PER_1000SQFT', y='SALESCLOSEPRICE', data=pandas_df, kind=\"reg\", stat_func=r2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Components\n",
    "\n",
    "Being able to work with time components for building features is important but you can also use them to explore and understand your data further. In this exercise, you'll be looking to see if there is a pattern to which day of the week a house lists on. Please keep in mind that PySpark's week starts on Sunday, with a value of 1 and ends on Saturday, a value of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import to_date, dayofweek\n",
    "\n",
    "# Convert to date type\n",
    "df = df.withColumn('LISTDATE', to_date('LISTDATE'))\n",
    "\n",
    "# Get the day of the week\n",
    "df = df.withColumn('List_Day_of_Week', dayofweek('LISTDATE'))\n",
    "\n",
    "# Sample and convert to pandas dataframe\n",
    "sample_df = df.sample(False, 0.5, 42).toPandas()\n",
    "\n",
    "# Plot count plot of of day of week\n",
    "ax = sns.countplot(x=\"List_Day_of_Week\", data=sample_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining On Time Components\n",
    "\n",
    "Often times you will use date components to join in other sets of information. However, in this example, we need to use data that would have been available to those considering buying a house. This means we will need to use the previous year's reporting data for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataframes\n",
    "df = real_estate_df\n",
    "price_df = median_prices_df\n",
    "\n",
    "# Create year column\n",
    "df = df.withColumn(\"list_year\", year(\"LISTDATE\"))\n",
    "\n",
    "# Adjust year to match\n",
    "df = df.withColumn(\"report_year\", (df[\"list_year\"] - 1))\n",
    "print(df)\n",
    "# Create join condition\n",
    "condition = [df['CITY'] == price_df['City'], df['report_year'] == price_df['Year']]\n",
    "\n",
    "# Join the dataframes together\n",
    "df = df.join(price_df, on=condition, how=\"left\")\n",
    "# Inspect that new columns are available\n",
    "df[['MedianHomeValue']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Math\n",
    "\n",
    "In this example, we'll look at verifying the frequency of our data. The Mortgage dataset is supposed to have weekly data but let's make sure by lagging the report date and then taking the difference of the dates.\n",
    "\n",
    "Recall that to create a lagged feature we will need to create a window(). window() allows you to return a value for each record based off some calculation against a group of records, in this case, the previous period's mortgage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, datediff, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Cast data type\n",
    "mort_df = mort_df.withColumn(\"Date\", to_date(\"DATE\"))\n",
    "\n",
    "# Create window\n",
    "w = Window().orderBy(mort_df[\"DATE\"])\n",
    "# Create lag column\n",
    "mort_df = mort_df.withColumn(\"DATE-1\", lag(\"DATE\", count=1).over(w))\n",
    "\n",
    "# Calculate difference between date columns\n",
    "mort_df = mort_df.withColumn(\"Days_Between_Report\", datediff(\"DATE\", \"DATE-1\"))\n",
    "# Print results\n",
    "mort_df.select('Days_Between_Report').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text to New Features\n",
    "\n",
    "Garages are an important consideration for houses in Minnesota where most people own a car and the snow is annoying to clear off a car parked outside. The type of garage is also important, can you get to your car without braving the cold or not? Let's look at creating a feature has_attached_garage that captures whether the garage is attached to the house or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import when \n",
    "\n",
    "# Create boolean conditions for string matches\n",
    "has_attached_garage = df['GARAGEDESCRIPTION'].like(\"%Attached Garage%\")\n",
    "has_detached_garage = df['GARAGEDESCRIPTION'].like(\"%Detached Garage%\")\n",
    "\n",
    "# Conditional value assignment \n",
    "df = df.withColumn(\"has_attached_garage\", (when(has_attached_garage, 1)\n",
    "                                          .when(has_detached_garage, 0)\n",
    "                                          .otherwise(None)))\n",
    "\n",
    "# Inspect results\n",
    "df[['GARAGEDESCRIPTION', 'has_attached_garage']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting & Exploding\n",
    "\n",
    "Being able to take a compound field like GARAGEDESCRIPTION and massaging it into something useful is an involved process. It's helpful to understand early what value you might gain out of expanding it. In this example, we will convert our string to a list-like array, explode it and then inspect the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import split , explode\n",
    "import pandas as pd\n",
    "# Convert string to list-like array\n",
    "df = df.withColumn(\"garage_list\", split(df['GARAGEDESCRIPTION'], \", \"))\n",
    "df.select([\"garage_list\",\"GARAGEDESCRIPTION\"]).show()\n",
    "\n",
    "# Explode the values into new records\n",
    "ex_df = df.withColumn(\"ex_garage_list\", explode(df[\"garage_list\"]))\n",
    "\n",
    "# Inspect the values\n",
    "ex_df[['ex_garage_list']].distinct().show(100, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot & Join\n",
    "\n",
    "Being able to explode and pivot a compound field is great, but you are left with a dataframe of only those pivoted values. To really be valuable you'll need to rejoin it to the original dataset! After joining the datasets we will have a lot of NULL values for the newly created columns since we know the context of how they were created we can safely fill them in with zero as either the new has an attribute or it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, first\n",
    "\n",
    "# Pivot \n",
    "piv_df = ex_df.groupBy('NO').pivot('ex_garage_list').agg(coalesce(first('constant_val')))\n",
    "\n",
    "# Join the dataframes together and fill null\n",
    "joined_df = df.join(piv_df, on='NO', how='left')\n",
    "\n",
    "# Columns to zero fill\n",
    "zfill_cols = piv_df.columns\n",
    "\n",
    "# Zero fill the pivoted values\n",
    "zfilled_df = joined_df.fillna(0, subset=zfill_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing Day of Week\n",
    "\n",
    "In a previous video, we saw that it was very unlikely for a home to list on the weekend. Let's create a new field that says if the house is listed for sale on a weekday or not. In this example there is a field called List_Day_of_Week that has Monday is labeled 1.0 and Sunday is 7.0. Let's convert this to a binary field with weekday being 0 and weekend being 1. We can use the pyspark feature transformer Binarizer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Create the transformer\n",
    "binarizer = Binarizer(threshold=5, inputCol=\"List_Day_of_Week\", outputCol=\"Listed_On_Weekend\")\n",
    "\n",
    "# Apply the transformation to df\n",
    "df = binarizer.transform(df)\n",
    "\n",
    "# Verify transformation\n",
    "df[[\"List_Day_of_Week\", \"Listed_On_Weekend\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing\n",
    "\n",
    "If you are a homeowner its very important if a house has 1, 2, 3 or 4 bedrooms. But like bathrooms, once you hit a certain point you don't really care whether the house has 7 or 8. This example we'll look at how to figure out where are some good value points to bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Plot distribution of sample_df\n",
    "sns.distplot(sample_df, axlabel='BEDROOMS')\n",
    "plt.show()\n",
    "\n",
    "# Create the bucket splits and bucketizer\n",
    "splits = [1, 2, 3, 4, 5, 6, float('Inf')]\n",
    "buck = Bucketizer(splits=splits, inputCol=\"BEDROOMS\", outputCol=\"bedrooms\")\n",
    "\n",
    "# Apply the transformation to df: df_bucket\n",
    "df_bucket = buck.transform(df)\n",
    "\n",
    "# Display results\n",
    "df_bucket[['BEDROOMS', 'bedrooms']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "In the United States where you live determines which schools your kids can attend. Therefore it's understandable that many people care deeply about which school districts their future home will be in. While the school districts are numbered in SCHOOLDISTRICTNUMBER they are really categorical. Meaning that summing or averaging these values has no apparent meaning. Therefore in this example we will convert SCHOOLDISTRICTNUMBER from a categorial variable into a numeric vector to use in our machine learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Map strings to numbers with string indexer\n",
    "string_indexer = StringIndexer(inputCol=\"SCHOOLDISTRICTNUMBER\", outputCol=\"School_Index\")\n",
    "indexed_df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "# Onehot encode indexed values\n",
    "encoder = OneHotEncoder(inputCol=\"School_Index\", outputCol=\"School_Vec\")\n",
    "encoded_df = encoder.transform(indexed_df)\n",
    "\n",
    "# Inspect the transformation steps\n",
    "encoded_df[['SCHOOLDISTRICTNUMBER', 'School_Index', 'School_Vec']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
