{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return spark version\n",
    "print(spark.version)\n",
    "\n",
    "# Return python version\n",
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a dataframe\n",
    "df = spark.read.parquet('Real_Estate.parq')\n",
    "# Print columns in dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our dependent variable\n",
    "Y_df = df.select([\"SALESCLOSEPRICE\"])\n",
    "\n",
    "# Display summary statistics\n",
    "Y_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Data Load\n",
    "\n",
    "Let's suppose each month you get a new file. You know to expect a certain number of records and columns. In this exercise we will create a function that will validate the file loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_load(df, num_records, num_columns):\n",
    "  # Takes a dataframe and compares record and column counts to input\n",
    "  # Message to return if the critera below aren't met\n",
    "  message = 'Validation Failed'\n",
    "  # Check number of records\n",
    "  if num_records == df.count():\n",
    "    # Check number of columns\n",
    "    if num_columns == len(df.columns):\n",
    "      # Success message\n",
    "      message = \"Validation Passed\"\n",
    "  return message\n",
    "\n",
    "# Print the data validation message\n",
    "print(check_load(df, 5000, 74))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying DataTypes\n",
    "\n",
    "In the age of data we have access to more attributes than we ever had before. To handle all of them we will build a lot of automation but at a minimum requires that their datatypes be correct. In this exercise we will validate a dictionary of attributes and their datatypes to see if they are correct. This dictionary is stored in the variable validation_dict and is available in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of actual dtypes to check\n",
    "actual_dtypes_list = df.dtypes\n",
    "print(actual_dtypes_list)\n",
    "\n",
    "# Iterate through the list of actual dtypes tuples\n",
    "for attribute_tuple in actual_dtypes_list:\n",
    "  \n",
    "  # Check if column name is dictionary of expected dtypes\n",
    "  col_name = attribute_tuple[0]\n",
    "  if col_name in validation_dict:\n",
    "\n",
    "    # Compare attribute types\n",
    "    col_type = attribute_tuple[1]\n",
    "    if col_type == validation_dict[col_name]:\n",
    "      print(col_name + ' has expected dtype.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Corr()\n",
    "\n",
    "The old adage 'Correlation does not imply Causation' is a cautionary tale. However, correlation does give us a good nudge to know where to start looking promising features to use in our models. Use this exercise to get a feel for searching through your data for the first time, trying to find patterns.\n",
    "\n",
    "A list called columns containing column names has been created for you. In this exercise you will compute the correlation between those columns and 'SALESCLOSEPRICE', and find the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(\"SALESCLOSEPRICE\", col)\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "        \n",
    "print(corr_max_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Visualizations: distplot\n",
    "\n",
    "Understanding the distribution of our dependent variable is very important and can impact the type of model or preprocessing we do. A great way to do this is to plot it, however plotting is not a built in function in PySpark, we will need to take some intermediary steps to make sure it works correctly. In this exercise you will visualize the variable the 'LISTPRICE' variable, and you will gain more insights on its distribution by computing the skewness.\n",
    "\n",
    "The matplotlib.pyplot and seaborn packages have been imported for you with aliases plt and sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({\"LISTPRICE\": \"skewness\"}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Visualizations: lmplot\n",
    "\n",
    "Creating linear model plots helps us visualize if variables have relationships with the dependent variable. If they do they are good candidates to include in our analysis. If they don't it doesn't mean that we should throw them out, it means we may have to process or wrangle them before they can be used.\n",
    "\n",
    "seaborn is available in your workspace with the customary alias sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['SALESCLOSEPRICE', 'LIVINGAREA']).sample(False, 0.5, 42)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Linear model plot of pandas_df\n",
    "sns.lmplot(x='LIVINGAREA', y='SALESCLOSEPRICE', data=pandas_df)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
