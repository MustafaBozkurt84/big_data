{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping a list of columns\n",
    "\n",
    "Our data set is rich with a lot of features, but not all are valuable. We have many that are going to be hard to wrangle into anything useful. For now, let's remove any columns that aren't immediately useful by dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 30 records\n",
    "df.show(30)\n",
    "\n",
    "# List of columns to remove from dataset\n",
    "cols_to_drop = ['STREETNUMBERNUMERIC', 'LOTSIZEDIMENSIONS']\n",
    "\n",
    "# Drop columns in list\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using text filters to remove records\n",
    "\n",
    "It pays to have to ask your clients lots of questions and take time to understand your variables. You find out that Assumable mortgage is an unusual occurrence in the real estate industry and your client suggests you exclude them. In this exercise we will use isin() which is similar to like() but allows us to pass a list of values to use as a filter rather than a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
    "df.select([\"ASSUMABLEMORTGAGE\"]).distinct().show()\n",
    "\n",
    "# List of possible values containing 'yes'\n",
    "yes_values = [\"Yes w/ Qualifying\", \"Yes w/No Assumable\"]\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)\n",
    "\n",
    "# Print count of remaining records\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outliers Filtering numeric fields conditionally\n",
    "\n",
    "Again, understanding the context of your data is extremely important. We want to understand what a normal range of houses sell for. Let's make sure we exclude any outlier homes that have sold for significantly more or less than the average. Here we will calculate the mean and standard deviation and use them to filer the near normal field log_SalesClosePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "df.sample(0.1,42).toPandas().columns\n",
    "# Calculate values used for outlier filtering\n",
    "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]\n",
    "\n",
    "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
    "low_bound = mean_val - (3 *stddev_val)\n",
    "hi_bound = mean_val + (3 *stddev_val)\n",
    "\n",
    "# Filter the data to fit between the lower and upper bounds\n",
    "df = df.filter((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Percentage Scaling\n",
    "\n",
    "In the slides we showed how to scale the data between 0 and 1. Sometimes you may wish to scale things differently for modeling or display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max and min values and collect them\n",
    "max_days = df.agg({\"DAYSONMARKET\": \"max\"}).collect()[0][0]\n",
    "min_days = df.agg({\"DAYSONMARKET\": \"min\"}).collect()[0][0]\n",
    "\n",
    "# Create a new column based off the scaled data\n",
    "df = df.withColumn(\"percentagesscaleddays\", \n",
    "                  round((df[\"DAYSONMARKET\"] - min_days) / (max_days - min_days)) * 100)\n",
    "\n",
    "# Calc max and min for new column\n",
    "print(df.agg({\"percentagesscaleddays\": \"max\"}).collect())\n",
    "print(df.agg({\"percentagesscaleddays\": \"min\"}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling your scalers\n",
    "\n",
    "In the previous exercise, we minmax scaled a single variable. Suppose you have a LOT of variables to scale, you don't want hundreds of lines to code for each. Let's expand on the previous exercise and make it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df, cols_to_scale):\n",
    "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "  for col in cols_to_scale:\n",
    "    # Define min and max values and collect them\n",
    "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "    new_column_name = 'scaled_' + col\n",
    "    # Create a new column based off the scaled data\n",
    "    df = df.withColumn(new_column_name, \n",
    "                      (df[col] - min_days) / (max_days - min_days))\n",
    "  return df\n",
    "  \n",
    "df = min_max_scaler(df, cols_to_scale)\n",
    "# Show that our data is now between 0 and 1\n",
    "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Right Skew Data\n",
    "\n",
    "In the slides we showed how you might use log transforms to fix positively skewed data (data whose distribution is mostly to the left). To correct negative skew (data mostly to the right) you need to take an extra step called \"reflecting\" before you can apply the inverse of log, written as (1/log) to make the data look more like normal a normal distribution. Reflecting data uses the following formula to reflect each value: (xmax+1)–x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9cc36a604826>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Compute the skewness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"YEARBUILT\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"skewness\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Calculate the max year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "# Compute the skewness\n",
    "print(df.agg({\"YEARBUILT\":\"skewness\"}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({\"YEARBUILT\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df[\"YEARBUILT\"])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
