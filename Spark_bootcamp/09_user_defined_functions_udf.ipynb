{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"User Defined Functions\") \\\n",
    ".master(\"local[2]\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 473232\r\n",
      "-rw-rw-r--. 1 train train  42658497 Nov  3 22:07 201508_trip_data.csv\r\n",
      "-rw-rw-r--. 1 train train      4556 Jul 21 18:58 Advertising.csv\r\n",
      "drwxrwxr-x. 2 train train      4096 Nov 10 22:02 cat_images\r\n",
      "-rw-rw-r--. 1 train train    674857 Sep 17 22:49 Churn_Modelling.csv\r\n",
      "drwxrwxr-x. 3 train train        96 Oct  6 12:18 churn-telecom\r\n",
      "-rw-rw-r--. 1 train train  41002480 Oct  6 12:18 Fire_Incidents.csv.gz\r\n",
      "drwxrwxr-x. 7 train train        67 Oct  6 12:18 flight-data\r\n",
      "-rw-rw-r--. 1 train train  46401315 Oct  6 12:18 Hotel_Reviews.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Sep 17 20:53 hotel_reviews_parquet\r\n",
      "-rw-rw-r--. 1 train train       180 Sep  2 11:40 insanlar.csv\r\n",
      "-rw-rw-r--. 1 train train      4611 Sep  1 16:13 iris.csv\r\n",
      "-rw-rw-r--. 1 train train     15802 Sep  1 16:13 iris.json\r\n",
      "-rw-rw-r--. 1 train train    325145 Sep  1 16:14 kuruyemisler.txt\r\n",
      "-rw-rw-r--. 1 train train  44525776 Oct  6 12:18 market1mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Nov 10 22:17 market1mil_snappyparquet\r\n",
      "-rw-rw-r--. 1 train train 269015852 Oct  6 12:18 market5mil.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       198 Nov 10 22:16 market5mil_snappyparquet\r\n",
      "-rw-rw-r--. 1 train train      7149 Sep  4 11:15 mixed_tc_kimlik_names_numbers.txt\r\n",
      "-rw-r--r--. 1 train train       844 Oct 25 10:25 movies.json\r\n",
      "-rw-rw-r--. 1 train train   1072063 Sep 29 14:58 online_shoppers_intention.csv\r\n",
      "-rw-rw-r--. 1 train train  38382007 Oct  6 12:18 pump_sensor_data.csv.gz\r\n",
      "drwxrwxr-x. 2 train train       133 Sep  2 12:13 retail_db\r\n",
      "-rw-rw-r--. 1 train train      7908 Sep  4 10:04 sample_tc_kimlik_names_numbers.txt\r\n",
      "drwxrwxr-x. 2 train train       112 Sep 29 18:35 sf-airbnb-clean.parquet\r\n",
      "-rw-rw-r--. 1 train train       223 Sep  2 13:03 simple_text.txt\r\n",
      "drwxrwxr-x. 2 train train        49 Oct  6 12:18 weblogs\r\n",
      "-rw-rw-r--. 1 train train    449173 Sep  3 22:32 words.txt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l ~/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    ".option(\"header\",True)\n",
    ".option(\"inferSchema\", True)\n",
    ".load(\"file:///home/train/datasets/flight-data/csv/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEST_COUNTRY_NAME</th>\n",
       "      <th>ORIGIN_COUNTRY_NAME</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>India</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Equatorial Guinea</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME  count\n",
       "0      United States             Romania      1\n",
       "1      United States             Ireland    264\n",
       "2      United States               India     69\n",
       "3              Egypt       United States     24\n",
       "4  Equatorial Guinea       United States      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"flight_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grp_by = spark.sql(\"\"\"\n",
    "\n",
    "SELECT ORIGIN_COUNTRY_NAME, SUM(count) AS Total_Count \n",
    "FROM flight_table \n",
    "GROUP BY ORIGIN_COUNTRY_NAME\n",
    "ORDER BY Total_Count DESC\n",
    "LIMIT 15\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|ORIGIN_COUNTRY_NAME|Total_Count|\n",
      "+-------------------+-----------+\n",
      "|      United States|    2352430|\n",
      "|             Canada|      49695|\n",
      "|             Mexico|      38225|\n",
      "|     United Kingdom|      10358|\n",
      "|              Japan|       8643|\n",
      "|            Germany|       8380|\n",
      "| Dominican Republic|       7194|\n",
      "|        The Bahamas|       5775|\n",
      "|             France|       5290|\n",
      "|           Colombia|       4981|\n",
      "|        South Korea|       4253|\n",
      "|            Jamaica|       4087|\n",
      "|              China|       4021|\n",
      "|        Netherlands|       3779|\n",
      "|             Brazil|       3427|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_grp_by.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(c):\n",
    "    return c.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ali'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case(\"ALi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.udf.register(name, f, returnType=None)\n",
    "lower_case_func = F.udf(lambda x: lower_case(x), StringType())\n",
    "\n",
    "spark.udf.register(\"lower_case_func\", lower_case_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|<lambda>(DEST_COUNTRY_NAME)|\n",
      "+---------------------------+\n",
      "|              united states|\n",
      "|              united states|\n",
      "|              united states|\n",
      "|                      egypt|\n",
      "|          equatorial guinea|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lower_case_func(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    One of the previous prevailing issues with using PySpark UDFs was that they had\n",
    "    slower performance than Scala UDFs. This was because the PySpark UDFs required\n",
    "    data movement between the JVM and Python, which was quite expensive. To resolve\n",
    "    this problem, Pandas UDFs (also known as vectorized UDFs) were introduced as part\n",
    "    of Apache Spark 2.3. A Pandas UDF uses Apache Arrow to transfer data and Pandas\n",
    "    to work with the data. You define a Pandas UDF using the keyword pandas_udf as\n",
    "    the decorator, or to wrap the function itself. Once the data is in Apache Arrow format,\n",
    "    there is no longer the need to serialize/pickle the data as it is already in a format\n",
    "    consumable by the Python process. Instead of operating on individual inputs row by\n",
    "    row, you are operating on a Pandas Series or DataFrame (i.e., vectorized execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_case(col: pd.Series) -> pd.Series:\n",
    "    return col.transform(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "# F.pandas_udf(function, returnType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_case_pdudf = F.pandas_udf(upper_case, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------+\n",
      "|DEST_COUNTRY_NAME|upper_case(DEST_COUNTRY_NAME)|\n",
      "+-----------------+-----------------------------+\n",
      "|    United States|                UNITED STATES|\n",
      "|    United States|                UNITED STATES|\n",
      "|    United States|                UNITED STATES|\n",
      "|            Egypt|                        EGYPT|\n",
      "|Equatorial Guinea|            EQUATORIAL GUINEA|\n",
      "+-----------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\",upper_case_pdudf(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
