{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Time Splits\n",
    "\n",
    "In the video, we learned why splitting data randomly can be dangerous for time series as data from the future can cause overfitting in our model. Often with time series, you acquire new data as it is made available and you will want to retrain your model using the newest data. In the video, we showed how to do a percentage split for test and training sets but suppose you wish to train on all available data except for the last 45days which you want to use for a test set.\n",
    "\n",
    "In this exercise, we will create a function to find the split date for using the last 45 days of data for testing and the rest for training. Please note that timedelta() has already been imported for you from the standard python library datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "  # Find how many days our data spans\n",
    "  max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
    "  min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
    "  # Subtract an integer number of days from the last date in dataset\n",
    "  split_date = max_date - timedelta(days=test_days)\n",
    "  return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, 'OFFMKTDATE')\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df['OFFMKTDATE'] < split_date)\n",
    "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting Time Features\n",
    "\n",
    "We have mentioned throughout this course some of the dangers of leaking information to your model during training. Data leakage will cause your model to have very optimistic metrics for accuracy but once real data is run through it the results are often very disappointing.\n",
    "\n",
    "In this exercise, we are going to ensure that DAYSONMARKET only reflects what information we have at the time of predicting the value. I.e., if the house is still on the market, we don't know how many more days it will stay on the market. We need to adjust our test_df to reflect what information we currently have as of 2017-12-10.\n",
    "\n",
    "NOTE: This example will use the lit() function. This function is used to allow single values where an entire column is expected in a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('2017-12-10'))\n",
    "# Create Sequential Test set\n",
    "test_df = df.where(df[\"OFFMKTDATE\"] >= split_date).where(df[\"LISTDATE\"] <= split_date)\n",
    "\n",
    "# Create a copy of DAYSONMARKET to review later\n",
    "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
    "\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn(\"DAYSONMARKET\", datediff(split_date, 'LISTDATE'))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'OFFMKTDATE', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Columns with Low Observations\n",
    "\n",
    "After doing a lot of feature engineering it's a good idea to take a step back and look at what you've created. If you've used some automation techniques on your categorical features like exploding or OneHot Encoding you may find that you now have hundreds of new binary features. While the subject of feature selection is material for a whole other course but there are some quick steps you can take to reduce the dimensionality of your data set.\n",
    "\n",
    "In this exercise, we are going to remove columns that have less than 30 observations. 30 is a common minimum number of observations for statistical significance. Any less than that and the relationships cause overfitting because of a sheer coincidence!\n",
    "\n",
    "NOTE: The data is available in the dataframe, df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_threshold = 30\n",
    "cols_to_remove = list()\n",
    "# Inspect first 10 binary columns in list\n",
    "for col in binary_cols[0:10]:\n",
    "  # Count the number of 1 values in the binary column\n",
    "  obs_count = df.agg({col: \"sum\"}).collect()[0][0]\n",
    "  # If less than our observation threshold, remove\n",
    "  if obs_count < obs_threshold:\n",
    "    cols_to_remove.append(col)\n",
    "    \n",
    "# Drop columns and print starting and ending dataframe shapes\n",
    "new_df = df.drop(*cols_to_remove)\n",
    "\n",
    "print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
    "print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naively Handling Missing and Categorical Values\n",
    "\n",
    "Random Forest Regression is robust enough to allow us to ignore many of the more time consuming and tedious data preparation steps. While some implementations of Random Forest handle missing and categorical values automatically, PySpark's does not. The math remains the same however so we can get away with some naive value replacements.\n",
    "\n",
    "For missing values since our data is strictly positive, we will assign -1. The random forest will split on this value and handle it differently than the rest of the values in the same feature.\n",
    "\n",
    "For categorical values, we can just map the text values to numbers and again the random forest will appropriately handle them by splitting on them. In this example, we will dust off pipelines from Introduction to PySpark to write our code more concisely. Please note that the exercise will start by displaying the dtypes of the columns in the dataframe, compare them to the results at the end of this exercise.\n",
    "\n",
    "NOTE: Pipeline and StringIndexer are already imported for you. The list categorical_cols is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values\n",
    "df = df.fillna(-1, subset=[\"WALKSCORE\",\"BIKESCORE\"])\n",
    "\n",
    "# Create list of StringIndexers using list comprehension\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "# Create pipeline of indexers\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "# Fit and Transform the pipeline to the original data\n",
    "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Clean up redundant columns\n",
    "df_indexed = df_indexed.drop(*categorical_cols)\n",
    "# Inspect data transformations\n",
    "print(df_indexed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression Model\n",
    "\n",
    "One of the great things about PySpark ML module is that most algorithms can be tried and tested without changing much code. Random Forest Regression is a fairly simple ensemble model, using bagging to fit. Another tree based ensemble model is Gradient Boosted Trees which uses a different approach called boosting to fit. In this exercise let's train a GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Train a Gradient Boosted Trees (GBT) model.\n",
    "gbt = GBTRegressor(featuresCol=\"features\",\n",
    "                           labelCol=\"SALESCLOSEPRICE\",\n",
    "                           predictionCol=\"Prediction_Price\",\n",
    "                           seed=42\n",
    "                           )\n",
    "\n",
    "# Train model.\n",
    "model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating & Comparing Algorithms\n",
    "\n",
    "Now that we've created a new model with GBTRegressor its time to compare it against our baseline of RandomForestRegressor. To do this we will compare the predictions of both models to the actual data and calculate RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", \n",
    "                                predictionCol=\"Prediction_Price\")\n",
    "# Dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "for key, preds in models.items():\n",
    "  # Create evaluation metrics\n",
    "  rmse = evaluator.evaluate(preds, {evaluator.metricName:\"rmse\"})\n",
    "  r2 = evaluator.evaluate(preds, {evaluator.metricName:\"r2\"})\n",
    "  \n",
    "  # Print Model Metrics\n",
    "  print(key + ' RMSE: ' + str(rmse))\n",
    "  print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Results\n",
    "\n",
    "It is almost always important to know which features are influencing your prediction the most. Perhaps its counterintuitive and that's an insight? Perhaps a hand full of features account for most of the accuracy of your model and you don't need to perform time acquiring or massaging other features.\n",
    "\n",
    "In this example we will be looking at a model that has been trained without any LISTPRICE information. With that gone, what influences the price the most?\n",
    "\n",
    "NOTE: The array of feature importances, importances has already been created for you from model.featureImportances.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature importances to a pandas column\n",
    "fi_df = pd.DataFrame(importances, columns=[\"importance\"]) #model.featureImportances.toArray()\n",
    "\n",
    "# Convert list of feature names to pandas column\n",
    "fi_df['feature'] = pd.Series(feature_cols)\n",
    "\n",
    "# Sort the data based on feature importance\n",
    "fi_df.sort_values(by=[\"importance\"], ascending=False, inplace=True)\n",
    "\n",
    "# Inspect Results\n",
    "fi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving & Loading Models\n",
    "\n",
    "Often times you may find yourself going back to a previous model to see what assumptions or settings were used when diagnosing where your prediction errors were coming from. Perhaps there was something wrong with the data? Maybe you need to incorporate a new feature to capture an unusual event that occurred?\n",
    "\n",
    "In this example, you will practice saving and loading a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
