{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are three ways you can get and set Spark properties.\n",
    "\n",
    "<h3>1. In SPARK_HOME changing following files</h3>\n",
    "<ul>\n",
    "    <li>log4j.properties.template</li>\n",
    "    <li>spark-defaults.conf.template</li>\n",
    "    <li>spark-env.sh.template</li>\n",
    "    </ul>\n",
    "    \n",
    "If you rename files by deleting template and change values in it spark uses these values.\n",
    "\n",
    "<h3>2. During the spark-submit</h3>\n",
    "<p>You can change configuration by passing the new values with --conf flag</p>\n",
    "<code>spark-submit --conf spark.sql.shuffle.partitions=5 --conf\n",
    "\"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/mainscala-\n",
    "chapter7_2.12-1.0.jar</code>\n",
    "\n",
    "<h3>3. During the creation and after creation of SparkSession</h3>\n",
    "<p>Some propertes are not configurable after spark session get created. So you can just modify isModifiable properties after sparkSession.</p>\n",
    "\n",
    "<code>spark = SparkSession.builder\n",
    ".config(\"spark.sql.shuffle.partitions\", 5)\n",
    ".config(\"spark.executor.memory\", \"2g\")\n",
    ".master(\"local[*]\")\n",
    ".appName(\"SparkConfig\")\n",
    ".getOrCreate()</code>\n",
    "\n",
    "<h3>How to see all properties of Spark Application</h3>\n",
    "<ul>\n",
    "    <li>spark.conf.getAll()</li>\n",
    "    <li>Spark UI Environment Tab</li>\n",
    "    </ul>\n",
    "    \n",
    "<h3>Commandline has precedence over configuration files and code</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scaling Spark for Large Workloads</h3>\n",
    "<p>To avoid job failures due to resource starvation\n",
    "or gradual performance degradation, there are a handful of Spark configurations that\n",
    "you can enable or alter. These configurations affect three Spark components: </p>\n",
    "<ul>\n",
    "    <li>Spark driver</li>\n",
    "    <li>Executor</li>\n",
    "    <li>Shuffle service running on the executor</li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "<h3>Dynamic Resource Allocation</h3>\n",
    "<code>spark.dynamicAllocation.enabled true\n",
    " spark.dynamicAllocation.minExecutors 2\n",
    " spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    " spark.dynamicAllocation.maxExecutors 20\n",
    " spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Spark executors’ memory and the shuffle service\n",
    "\n",
    "<p>Although we use dynamic allocation we have to define a single executor memory and cpu cores. Dynamic resource allocation just gets and leaves executors. We cannot change executors memory or cpu core, what changes is only number of executors.</p>\n",
    "\n",
    "<p>Configuring Spark executors’ memory and the shuffle service.</p>\n",
    "\n",
    "<img src=\"../images/spark_memory_management-spark_yarn_memory_eng.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Execution memory is used for Spark shuffles, joins, sorts, and aggregations. Since different\n",
    "queries may require different amounts of memory, the fraction <strong>(spark.mem\n",
    "ory.fraction is 0.6 by default)</strong> of the available memory to dedicate to this can be\n",
    "tricky to tune but it’s easy to adjust. By contrast, storage memory is primarily used for\n",
    "caching user data structures and partitions derived from DataFrames.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recomended properties for shuffle ops based on experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_recomended_props_for_shuffle_ops.jpg\"/>\n",
    "\n",
    "<p>Source: Learning Spark, O'Reilly, 2020</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Default system properties included when running spark-submit.\r\n",
      "# spark.master                     spark://master:7077\r\n",
      "# spark.eventLog.enabled           true\r\n",
      "# spark.eventLog.dir               hdfs://namenode:8021/directory\r\n",
      "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\r\n",
      "# spark.driver.memory              5g\r\n",
      "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"\r\n",
      "spark.driver.memory                         512m\r\n",
      "spark.shuffle.file.buffer                   1m\r\n",
      "spark.file.transferTo                       false\r\n",
      "spark.shuffle.unsafe.file.output.buffer     1m\r\n",
      "spark.io.compression.lz4.blockSize          512k\r\n",
      "spark.shuffle.service.index.cache.size      200m\r\n",
      "spark.shuffle.registration.timeout          12000ms\r\n",
      "spark.shuffle.registration.maxAttempts      5\r\n",
      "spark.sql.warehouse.dir                     /user/hive/warehouse\r\n"
     ]
    }
   ],
   "source": [
    "! cat /opt/manual/spark/conf/spark-defaults.conf | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/spark_tasks_core_partitions.png\"/>\n",
    "\n",
    "Source: Learning Spark, O'Reilly, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How partitions are created?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>A contiguous collection of these blocks constitutes a partition. For example HDSF block size: 128 MB</li>\n",
    "    <li>Size of partitions: <code>spark.sql.files.maxPartitionBytes</code></li>\n",
    "    <li>You can change partition number with repartition(n) during the read or after <code>df.repartition(20)</code></li>\n",
    "    <li>Finally, shuffle partitions are created during the shuffle stage. By default, the number\n",
    "of shuffle partitions is set to <code>200</code> in spark.sql.shuffle.partitions. You can <code>adjust this number depending on the size of the data set</code> you have, to reduce the amount of\n",
    "small partitions being sent across the network to executors’ tasks. <code>lower value such as the number of cores on the executors or less</code></li>  \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvspark",
   "language": "python",
   "name": "venvspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
